{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkSVIUMSRDqa"
   },
   "source": [
    "# Simion Cristian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56pYQfBJdhR9"
   },
   "source": [
    "# Incarcare module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kz-KBxRcUiE",
    "outputId": "a524ae02-db6d-41b4-b574-c718d1f1ce9c"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYnt0LuSeqpc",
    "outputId": "a9d24b92-c0a1-4784-a064-700159e94366"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/alexlee-gk/lpips-tensorflow.git\n",
    "!cd lpips-tensorflow && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BxjbHuYRZeZ",
    "outputId": "f281e451-9144-4eb8-c3e9-3ebca4ce06fc"
   },
   "outputs": [],
   "source": [
    "# Functii de sistem\n",
    "import os, sys\n",
    "\n",
    "# Functii din PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Pentru setul de date\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Pentru debugging\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "# Biblioteci standard de prelucrare\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bara de progres\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Operatii cu imagini\n",
    "import imageio \n",
    "from skimage.transform import rescale\n",
    "\n",
    "# Tensorflow (pentru metrici)\n",
    "import tensorflow as tf\n",
    "%tensorflow_version 1.x\n",
    "# Cale pentru LPIPS\n",
    "sys.path.append('/content/lpips-tensorflow')\n",
    "import lpips_tf\n",
    "\n",
    "# Operatii cu json\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epINW62vAVbv",
    "outputId": "1ed0674c-c6df-41b3-dc44-b318d5229067"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFzsE67HSOZP",
    "outputId": "df0b8f1b-6f12-426c-8b3f-3e27bdbef3e7"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD-weF0Zd-Aw"
   },
   "source": [
    "# Incarcarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keKpRiwwF6oP"
   },
   "outputs": [],
   "source": [
    "class TinyNerfDataset(Dataset):\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "\n",
    "        data = np.load(os.path.join(dir, 'tiny_nerf_data.npz'))\n",
    "        images = torch.from_numpy(data['images'])[:100]\n",
    "        poses = torch.from_numpy(data['poses'])[:100]\n",
    "        focal = data['focal']\n",
    "\n",
    "        H, W = images.shape[1:3]\n",
    "\n",
    "        # (len(images) * W * H, 3)\n",
    "        self.rgbs = images.view((-1, 3))\n",
    "        # (len(poses), W * H, 2, 3)\n",
    "        self.rays = [extract_rays(H, W, focal, pose) for pose in poses]\n",
    "        self.rays = torch.cat(self.rays)\n",
    "        self.rays = self.rays.view((-1, 2, 3))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.rgbs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = { \"ray\": self.rays[idx], \"rgb\": self.rgbs[idx] }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAY7hR31aE-G"
   },
   "outputs": [],
   "source": [
    "class BlenderRaysDataset(Dataset):\n",
    "    def __init__(self, dir, downscale_factor=2, role='train', blend_alpha=True):\n",
    "        \"\"\"\n",
    "            dir: folder-ul in care se afla datele:\n",
    "                    un alt folder cu numele tipului + \n",
    "                    un fisier transforms_{role}.json)\n",
    "            downscale_factor: factorul cu care se micsoreaza rezolutia\n",
    "            role: un string care indica tipul setului de date, poate fi:\n",
    "                ['train', 'val', 'test']\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        with open(os.path.join(dir, 'transforms_' + role + '.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        images = []\n",
    "        poses = []\n",
    "\n",
    "        for i, frame in enumerate(metadata['frames']):\n",
    "            path = os.path.join(dir, frame['file_path'] + '.png')\n",
    "            raw_img = imageio.imread(path)\n",
    "\n",
    "            if np.isclose(downscale_factor, 1.):\n",
    "                img = np.clip(raw_img / 255, 0., 1.).astype(np.float32)\n",
    "            else:\n",
    "                # Obs: channel_axis=-1 in loc multichannel pentru skimage modern\n",
    "                img = rescale(raw_img, scale = 1 / downscale_factor, \n",
    "                                multichannel=True).astype(np.float32)\n",
    "            images.append(img)\n",
    "        \n",
    "            pose = np.array(frame['transform_matrix'], dtype=np.float32)\n",
    "            poses.append(pose)\n",
    "\n",
    "            # Folosim un set mai mic de date pentru validare\n",
    "            if role != 'train' and i == 30 - 1:\n",
    "                print(\"Trimming non-training data\")\n",
    "                break\n",
    "        \n",
    "        # torch.tensor(np.array(LM)) mult mai rapid decat torch.tensor(LM)\n",
    "        # daca LM este o lista de matrice numpy\n",
    "        # https://github.com/pytorch/pytorch/issues/13918\n",
    "        images = torch.tensor(np.array(images))\n",
    "        self.poses = torch.tensor(np.array(poses))\n",
    "\n",
    "        H, W = images[0].shape[:2]\n",
    "\n",
    "        fov_x = metadata['camera_angle_x']\n",
    "        self.focal = W / (2 * np.tan(fov_x / 2))\n",
    "        print(self.focal)\n",
    "\n",
    "        if blend_alpha:\n",
    "            rgb = images[..., :3]\n",
    "            alpha = images[..., -1:]\n",
    "            images = alpha * rgb + (1. - alpha)\n",
    "        else:\n",
    "            images = images[..., :3]\n",
    "        \n",
    "        # (len(images) * W * H, 3)\n",
    "        self.rgbs = images.view((-1, 3))\n",
    "        # (len(poses), W * H, 2, 3)\n",
    "        self.rays = [extract_rays(H, W, self.focal, pose) for pose in self.poses]\n",
    "        self.rays = torch.cat(self.rays)\n",
    "        self.rays = self.rays.view((-1, 2, 3))\n",
    "\n",
    "        print(self.rgbs.shape, self.rays.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.rgbs.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = { \"ray\": self.rays[idx], \"rgb\": self.rgbs[idx] }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXoEjXfe_iEp"
   },
   "outputs": [],
   "source": [
    "class BlenderImageDataset(Dataset):\n",
    "    def __init__(self, dir, downscale_factor=2, role='train', blend_alpha=True):\n",
    "        \"\"\"\n",
    "            dir: folder-ul in care se afla datele:\n",
    "                    un alt folder cu numele tipului + \n",
    "                    un fisier transforms_{role}.json)\n",
    "            downscale_factor: factorul cu care se micsoreaza rezolutia\n",
    "            role: un string care indica tipul setului de date, poate fi:\n",
    "                ['train', 'val', 'test']\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "        with open(os.path.join(dir, 'transforms_' + role + '.json'), 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "\n",
    "        images = []\n",
    "        poses = []\n",
    "\n",
    "        for frame in metadata['frames']:\n",
    "            path = os.path.join(dir, frame['file_path'] + '.png')\n",
    "            raw_img = imageio.imread(path)\n",
    "\n",
    "            if np.isclose(downscale_factor, 1.):\n",
    "                img = np.clip(raw_img / 255, 0., 1.).astype(np.float32)\n",
    "            else:\n",
    "                # Obs: channel_axis=-1 in loc multichannel pentru skimage modern\n",
    "                img = rescale(raw_img, scale = 1 / downscale_factor, \n",
    "                            multichannel=True).astype(np.float32)\n",
    "            images.append(img)\n",
    "        \n",
    "            pose = np.array(frame['transform_matrix'], dtype=np.float32)\n",
    "            poses.append(pose)\n",
    "        \n",
    "        # torch.tensor(np.array(LM)) mult mai rapid decat torch.tensor(LM)\n",
    "        # daca LM este o lista de matrice numpy\n",
    "        # https://github.com/pytorch/pytorch/issues/13918\n",
    "        images = torch.tensor(np.array(images))\n",
    "        self.poses = torch.tensor(np.array(poses))\n",
    "\n",
    "        H, W = images[0].shape[:2]\n",
    "\n",
    "        fov_x = metadata['camera_angle_x']\n",
    "        focal = W / (2 * np.tan(fov_x / 2))\n",
    "        print(focal)\n",
    "\n",
    "        if blend_alpha:\n",
    "            rgb = images[..., :3]\n",
    "            alpha = images[..., -1:]\n",
    "            images = alpha * rgb + (1. - alpha)\n",
    "        else:\n",
    "            images = images[..., :3]\n",
    "        \n",
    "        self.images = images\n",
    "        # hwf = (Height, Width, Focal)\n",
    "        self.hwf = (H, W, focal)\n",
    "\n",
    "        print(self.images.shape, self.poses.shape, self.hwf)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = { \n",
    "            \"image\": self.images[idx], \n",
    "            \"pose\": self.poses[idx], \n",
    "            \"hwf\": self.hwf\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vHsacVjvirK"
   },
   "source": [
    "## LLFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy-kLijPtxD0"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / np.linalg.norm(x, axis=0, keepdims=True)\n",
    "\n",
    "def poses_average(poses):\n",
    "    # Extram translatia medie: (1, 4)\n",
    "    mean_center = np.mean(poses[..., -1], axis=0)\n",
    "    \n",
    "    # Extragem axele y, z medii: (1, 4)\n",
    "    mean_z = np.mean(poses[..., 2], axis=0)\n",
    "    mean_y = np.mean(poses[..., 1], axis=0)\n",
    "\n",
    "    # Deducem noile directii: (1, 4)\n",
    "    new_x = np.cross(mean_y, mean_z)\n",
    "    new_y = np.cross(mean_z, new_x)\n",
    "\n",
    "    # Normalizam vectorii\n",
    "    x = normalize(new_x)\n",
    "    y = normalize(new_y)\n",
    "    z = normalize(mean_z)\n",
    "\n",
    "    avg_pose = np.column_stack((x, y, z, mean_center))\n",
    "\n",
    "    return avg_pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rw8mxx0XyL7T"
   },
   "outputs": [],
   "source": [
    "def recenter_poses(poses):\n",
    "    # Pentru a centra pose-urile, inmultim inversa matricei pose medii cu \n",
    "    # fiecare matrice pose in parte\n",
    "    avg_pose = poses_average(poses)\n",
    "    ones = np.array([0., 0., 0., 1.])\n",
    "    avg_pose = np.vstack((avg_pose, ones))\n",
    "\n",
    "    # Pentru a concatena pe o axa, trebuie sa am aceeasi forma pe celelalte\n",
    "    ones_expanded = np.broadcast_to(ones, (poses.shape[0], 1, 4))\n",
    "    poses_square = np.concatenate((poses, ones_expanded), axis=1)\n",
    "\n",
    "    inv_avg = np.linalg.inv(avg_pose)\n",
    "    centered_poses = inv_avg @ poses_square\n",
    "\n",
    "    return centered_poses[..., :3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdHeL691s4HQ"
   },
   "outputs": [],
   "source": [
    "def convert_to_ndc(H, W, focal, near, rays):\n",
    "    o, d = rays[:, 0, :], rays[:, 1, :]\n",
    "\n",
    "    # t = -(n + o_z) / d_z, (num_rays)\n",
    "    t = -(near + o[..., 2]) / d[..., 2]\n",
    "    o = o + t.unsqueeze(-1) * d\n",
    "\n",
    "    ox, oy, oz = torch.split(o, 1, dim=-1)\n",
    "    dx, dy, dz = torch.split(d, 1, dim=-1)\n",
    "\n",
    "    x_coef = -focal / (W / 2)\n",
    "    y_coef = -focal / (H / 2)\n",
    "    \n",
    "    new_ox = x_coef * ox / oz\n",
    "    new_oy = y_coef * oy / oz\n",
    "    new_oz = 1. + 2 * near / oz\n",
    "\n",
    "    new_dx = x_coef * (dx / dz - ox / oz)\n",
    "    new_dy = y_coef * (dy / dz - oy / oz)\n",
    "    new_dz = -2. * near / oz\n",
    "\n",
    "    # (num_rays, 3)\n",
    "    new_o = torch.hstack((new_ox, new_oy, new_oz))\n",
    "    new_d = torch.hstack((new_dx, new_dy, new_dz))\n",
    "\n",
    "    # (num_rays, 2, 3)\n",
    "    rays_ndc = torch.stack((new_o, new_d), dim=1)\n",
    "\n",
    "    return rays_ndc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7z23lfP4P1O"
   },
   "outputs": [],
   "source": [
    "class LLFFDataset(Dataset):\n",
    "    def __init__(self, dir, downscale_factor=8, role='train', \n",
    "                 bounds_factor=0.75):\n",
    "        \"\"\"\n",
    "            dir: folder-ul in care se afla datele:\n",
    "                    un fisier poses_bounds.npy + \n",
    "                    un fisier images_{role}.json)\n",
    "            downscale_factor: factorul cu care se micsoreaza rezolutia\n",
    "            role: un string care indica tipul setului de date, poate fi:\n",
    "                ['train', 'val', 'test']\n",
    "        \"\"\"\n",
    "        self.dir = dir\n",
    "\n",
    "        # (num_images, 17)\n",
    "        poses_bounds = np.load(os.path.join(dir, 'poses_bounds.npy'))\n",
    "        # poses_bounds = np.array([pb for i, pb in enumerate(poses_bounds) \n",
    "                                        # if i % i_skip == 0])\n",
    "        num_images = poses_bounds.shape[0]\n",
    "\n",
    "        # (num_images, 3, 5)\n",
    "        poses_hwf = poses_bounds[..., :15].reshape(-1, 3, 5)\n",
    "\n",
    "        # (num_images, 3, 4) (num_images, 3, 1)\n",
    "        poses, hwf = np.split(poses_hwf, [-1], axis=-1)\n",
    "\n",
    "        H, W, focal = hwf[0, ..., 0]\n",
    "\n",
    "        # (num_images, 2)\n",
    "        near, far = np.split(poses_bounds[..., -2:], [1], axis=-1)\n",
    "\n",
    "        # Extrage o lista cu numele imaginilor, in ordinea in care se afla\n",
    "        # si matricele pose\n",
    "        prefix = os.path.join(dir, 'images')\n",
    "        scaled = False\n",
    "        if os.path.exists(prefix + '_' + str(downscale_factor)):\n",
    "            prefix = prefix + '_' + str(downscale_factor)\n",
    "            scaled = True\n",
    "\n",
    "        _, _, files = next(os.walk(prefix))\n",
    "        # Sortarea este importanta, matricele pose sunt in ordinea numelor\n",
    "        files.sort()\n",
    "        \n",
    "        images = []\n",
    "\n",
    "        for i, fname in enumerate(files):\n",
    "            print(fname)\n",
    "            img = imageio.imread(os.path.join(prefix, fname))\n",
    "\n",
    "            if not scaled:\n",
    "                # TODO: channel_axis=-1 in loc multichannel pentru skimage modern\n",
    "                img = rescale(img, scale = 1 / downscale_factor, \n",
    "                                multichannel=True).astype(np.float32)\n",
    "            else:\n",
    "                img = np.clip(img / 255, 0., 1.).astype(np.float32)\n",
    "            \n",
    "            images.append(img)\n",
    "       \n",
    "        assert num_images == len(images), \\\n",
    "            \"Numarul de imagini nu este egal cu numarul de matrici pose!\"\n",
    "\n",
    "        # Dimensiunile citite din poses_bounds nu sunt neaparat corecte, daca\n",
    "        # utilizam rezolutii mai mici\n",
    "        H, W = images[0].shape[:2]\n",
    "        self.focal = focal / downscale_factor\n",
    "        self.hwf = (H, W, self.focal)\n",
    "        self.role = role\n",
    "\n",
    "        # LLFF foloseste sistemul [down, right, back], schimbam catre \n",
    "        # [right, up, back] cu rotatia in jurul axei Z la -90 de grade\n",
    "        # Interschimb coloanele 1 si 2, iar coloana 2 are semnul inversat\n",
    "        # (num_images, 3, 4)\n",
    "        poses = poses @ np.array([\n",
    "            [ 0., 1., 0., 0.],\n",
    "            [-1., 0., 0., 0.],\n",
    "            [ 0., 0., 1., 0.],\n",
    "            [ 0., 0., 0., 1.]]).T\n",
    "\n",
    "        # Scalam limitele scenei astfel incat \"near\" sa fie putin peste 1\n",
    "        bounds_scale = 1. / (near.min() * bounds_factor)\n",
    "        near = near * bounds_scale\n",
    "        far = far * bounds_scale\n",
    "        poses[..., 3] *= bounds_scale\n",
    "\n",
    "        # Modificam pose-urile astfel incat orientarea scenei sa fie normalizata\n",
    "        self.avg_pose = poses_average(poses)\n",
    "        poses = recenter_poses(poses)\n",
    "\n",
    "        i_skip = 8\n",
    "        if role == 'train':\n",
    "            mask = np.ones(num_images, dtype=np.bool)\n",
    "            mask[::i_skip] = False\n",
    "        else:\n",
    "            mask = np.zeros(num_images, dtype=np.bool)\n",
    "            mask[::i_skip] = True\n",
    "\n",
    "        self.images = torch.tensor(np.array(images)[mask])\n",
    "        self.poses = torch.tensor(poses[mask].astype(np.float32))\n",
    "\n",
    "        if role == 'train':\n",
    "            # (num_images * W * H, 3)\n",
    "            self.rgbs = self.images.view((-1, 3))\n",
    "            # (num_images, W * H, 2, 3)\n",
    "            self.rays = [extract_rays(H, W, self.focal, pose) for pose in self.poses]\n",
    "            self.rays = torch.cat(self.rays)\n",
    "            self.rays = self.rays.view((-1, 2, 3))\n",
    "\n",
    "            self.rays = convert_to_ndc(H, W, self.focal, 1., self.rays)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.role == 'train':\n",
    "            return self.rgbs.shape[0]\n",
    "        else:\n",
    "            return self.images.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.role == 'train':\n",
    "            sample = { \"ray\": self.rays[idx], \"rgb\": self.rgbs[idx] }\n",
    "            return sample\n",
    "        \n",
    "        sample = { \n",
    "            \"image\": self.images[idx], \n",
    "            \"pose\": self.poses[idx], \n",
    "            \"hwf\": self.hwf\n",
    "            }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vtrbenu-w71W"
   },
   "source": [
    "# Functii principale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBsgtxE4P2JY"
   },
   "outputs": [],
   "source": [
    "def extract_rays(H, W, focal, cam_to_world):\n",
    "    # Cream un grid pentru a evalua in fiecare punct\n",
    "    y, x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
    "\n",
    "    # Centram vectorii, inversam directia y deoarece vrem sa creasca cand urcam\n",
    "    # in sus, directia z este invers ochiului\n",
    "    directions = torch.stack([ (x - W / 2) / focal, \n",
    "                              -(y - H / 2) / focal,\n",
    "                              -torch.ones_like(x)], -1)\n",
    "\n",
    "    # Aducem directiile razelor din spatiul camerei in spatiul obiect prin rotatie\n",
    "    # rays_dir = np.sum(np.reshape(directions, (H, W, 1, -1)) * cam_to_world[:3, :3], -1)\n",
    "    rays_dir = cam_to_world[:3, :3] @ torch.unsqueeze(directions, -1)\n",
    "    rays_dir = rays_dir.view((-1, 3))\n",
    "    \n",
    "    # Normalizam vectorii de directie\n",
    "    rays_dir = rays_dir / torch.linalg.norm(rays_dir, dim=-1, keepdim=True)\n",
    "\n",
    "    # Ultima coloana din matrice reprezinta translatiile la origine\n",
    "    rays_origin = torch.broadcast_to(cam_to_world[:3, -1], rays_dir.shape)\n",
    "\n",
    "    # (H * W, 2, 3)\n",
    "    rays = torch.stack([rays_origin, rays_dir], dim=1)\n",
    "    return rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "emtZ1NE-dWlr"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(x, embedding_size, keep_original=False):\n",
    "    def sin_cos(p, L):\n",
    "        val = 2 ** L * p \n",
    "        return torch.cat([torch.sin(val), torch.cos(val)], dim=1)\n",
    "    \n",
    "    if keep_original:\n",
    "        penc = x.detach().clone()\n",
    "    else:\n",
    "        penc = torch.tensor([], device=device)\n",
    "\n",
    "    for i in range(embedding_size):\n",
    "        penc = torch.cat([penc, sin_cos(x, i)], dim=1)\n",
    "\n",
    "    return penc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kXxbrt1_4MtA"
   },
   "outputs": [],
   "source": [
    "def process_chunk(model_func, pts_flat, dir_flat, emb_opts):\n",
    "    # Extrag informatiile despre dimensiuni din optiunile de embedding\n",
    "    emb_size_pos, emb_size_dir, keep_original = emb_opts\n",
    "    \n",
    "    # pts_flat si dir_flat au forma (chunk, num_samples, 3)\n",
    "    pts_emb = embed_func(pts_flat, emb_size_pos, keep_original)\n",
    "    dir_emb = embed_func(dir_flat, emb_size_dir, keep_original)\n",
    "\n",
    "    inputs = torch.cat((pts_emb, dir_emb), dim=-1)\n",
    "\n",
    "    return model_func(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0IKB1AXgXzX"
   },
   "outputs": [],
   "source": [
    "def get_weights_from_samples(model_func, rays, \n",
    "        z_coords, num_samples, emb_opts, regularize=0., chunk=None):\n",
    "    \n",
    "    b_size = rays.shape[0]\n",
    "    rays_origin, rays_dir = rays[:, 0, :], rays[:, 1, :]\n",
    "\n",
    "     # (b_size, 3) -> (b_size, num_samples, 3)\n",
    "    points = rays_origin.unsqueeze(1) + rays_dir.unsqueeze(1) * z_coords.unsqueeze(2)\n",
    "\n",
    "    # Extindem dimensiunea directiilor\n",
    "    # (b_size, 3) -> (b_size, num_samples, 3)\n",
    "    view_dir = torch.broadcast_to(rays_dir.unsqueeze(1), points.shape)\n",
    "    \n",
    "    # Redimensionam punctele si directiile\n",
    "    # (b_size * num_samples, 3)\n",
    "    points = torch.reshape(points, (-1, 3))\n",
    "    view_dir = torch.reshape(view_dir, (-1, 3))\n",
    "\n",
    "    # (W * H * num_samples, 4)\n",
    "    # outputs = batch_func(model_func, chunk=1024*32)(inputs)\n",
    "\n",
    "    if chunk is None:\n",
    "        outputs = process_chunk(model_func, points, view_dir, emb_opts)\n",
    "    else:\n",
    "        outputs = []\n",
    "        for i in range(0, points.shape[0], chunk):\n",
    "            # with profiler.profile(record_shapes=True, profile_memory=True, use_cuda=True) as prof:\n",
    "            #     with profiler.record_function(\"model_inference\"):\n",
    "            \n",
    "            outputs += [process_chunk(model_func, points[i:i+chunk], \n",
    "                view_dir[i:i+chunk], emb_opts)]\n",
    "\n",
    "            # print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cuda_memory_usage\", \n",
    "                                                                #  row_limit=20))\n",
    "        outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "        # dot = torchviz.make_dot(outputs, params=dict(model.named_parameters()))\n",
    "        # return dot\n",
    "        # prof.export_chrome_trace(\"trace.json\")\n",
    "    \n",
    "    outputs = torch.reshape(outputs, (b_size, num_samples, 4))\n",
    "\n",
    "    # (b_size, num_samples, 3)\n",
    "    rgb = torch.sigmoid(outputs[..., :3])\n",
    "    # (b_size, num_samples)\n",
    "    sigma = outputs[..., 3]\n",
    "    if regularize > 0.:\n",
    "        random_noise = torch.normal(mean=0., std=regularize, size=sigma.shape)\n",
    "        sigma = sigma + random_noise.to(device)\n",
    "    sigma = F.softplus(sigma)\n",
    "    \n",
    "    # (b_size, num_samples)\n",
    "    dists = torch.cat((z_coords[..., 1:] - z_coords[..., :-1], \n",
    "                torch.broadcast_to(\n",
    "                    torch.tensor([1e10], device=device), z_coords[..., :1].shape)), \n",
    "                dim=-1)\n",
    "\n",
    "    # (b_size, num_samples)\n",
    "    alpha = 1. - torch.exp(-sigma * dists)\n",
    "\n",
    "    # in tensorflow exista varianta exclusive care este simulata aici\n",
    "    # (b_size, num_samples)\n",
    "    weights = alpha * torch.cumprod(\n",
    "        torch.cat(\n",
    "            (torch.ones((b_size, 1), device=device), 1. - alpha + 1e-10), dim=-1), \n",
    "        dim=-1)[..., :-1]\n",
    "\n",
    "    return weights, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_P-nzQ_cU4zY"
   },
   "outputs": [],
   "source": [
    "def get_fine_samples(bins, weights, num_samples_fine, eps=1e-5, stratify=False):\n",
    "    b_size, num_samples_coarse = weights.shape\n",
    "\n",
    "    # (b_size, num_samples_coarse-2)\n",
    "    w_nonzero = weights + eps\n",
    "    density_weights = w_nonzero / torch.sum(w_nonzero, dim=-1, keepdim=True)\n",
    "   \n",
    "    # (b_size, num_samples_coarse-2)\n",
    "    cum_dens = torch.cumsum(density_weights, dim=-1)\n",
    "    # (b_size, num_samples_coarse-1)\n",
    "    cum_dens = torch.cat([torch.zeros((b_size, 1), device=device), cum_dens], \n",
    "                         dim=-1)\n",
    "\n",
    "    if stratify:\n",
    "        U = torch.rand((b_size, num_samples_fine), device=device)\n",
    "    else:\n",
    "        U = torch.linspace(0, 1, num_samples_fine, device=device)\n",
    "        U = U.expand((b_size, num_samples_fine)).contiguous()\n",
    "\n",
    "    # Noii indici, multi se afla la aceleasi praguri, vom interpola intre\n",
    "    # valorile la care se schimba treptele\n",
    "    # (b_size, num_samples_fine)\n",
    "    fine_idx = torch.searchsorted(cum_dens, U, right=True)\n",
    "\n",
    "    # Cautam pragurile inferioare si superioare\n",
    "    # (b_size, num_samples_fine)\n",
    "    below = torch.maximum(fine_idx - 1, torch.tensor(0))\n",
    "    above = torch.minimum(fine_idx, torch.tensor(num_samples_coarse))\n",
    "\n",
    "    # Combinam indicii pentru a cauta cu o singura functie: \n",
    "    # (b_size, 2 * num_samples_fine)\n",
    "    inds_step = torch.stack([below, above], dim=-1).view(b_size, -1)\n",
    "\n",
    "    # Extragem valorile aflate la indicii de la ambele praguri\n",
    "    # (b_size, num_samples_fine, 2)\n",
    "    cum_dens_step = torch.gather(cum_dens, 1, inds_step) \\\n",
    "                            .view(b_size, num_samples_fine, -1)\n",
    "    bins_step = torch.gather(bins, 1, inds_step) \\\n",
    "                            .view(b_size, num_samples_fine, -1)\n",
    "\n",
    "    # Diferentele intre pragurile de sus si jos\n",
    "    # (b_size, num_samples_fine)\n",
    "    delta = cum_dens_step[..., 1] - cum_dens_step[..., 0]\n",
    "    # Nu e nevoie sa scalam intre valorile foarte apropiate de 0\n",
    "    delta[delta < eps] = 1\n",
    "\n",
    "    # Parametrul de interpolare liniara pentru fiecare i, scaland diferenta\n",
    "    # fiecarei trepte cu inaltimea dintre praguri\n",
    "    # (b_size, num_samples_fine)\n",
    "    t = (U - cum_dens_step[..., 0]) / delta\n",
    "    \n",
    "    # Valorile repetate dintre praguri vor fi interpolate liniar\n",
    "    # (b_size, num_samples_fine)\n",
    "    samples = bins_step[..., 0] + t * (bins_step[..., 1] - bins_step[..., 0])\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5UdHUIpfmm0"
   },
   "outputs": [],
   "source": [
    "def render_rays(models, rays, params):\n",
    "    # Extragem parametrii\n",
    "    near, far, num_samples_coarse, num_samples_fine, emb_opts, \\\n",
    "      blend_alpha, stratify, regularize, chunk = params\n",
    "\n",
    "    b_size = rays.shape[0]\n",
    "\n",
    "    if stratify:\n",
    "        # Discretizam intervalul cu un interval in plus\n",
    "        z_coords = torch.linspace(near, far, num_samples_coarse + 1, device=device)\n",
    "        z_coords = z_coords.expand((b_size, num_samples_coarse + 1))\n",
    "\n",
    "        t = torch.rand((b_size, num_samples_coarse), device=device)\n",
    "        deltas = z_coords[..., 1:] - z_coords[..., :-1]\n",
    "        z_coords = z_coords[..., :-1]  + deltas * t\n",
    "    else:\n",
    "        # Discretizam intervalul\n",
    "        z_coords = torch.linspace(near, far, num_samples_coarse, device=device)\n",
    "        z_coords = z_coords.expand((b_size, num_samples_coarse))\n",
    "\n",
    "    weights, rgb = get_weights_from_samples(models[0], rays, z_coords, \n",
    "                                num_samples_coarse, emb_opts, regularize, chunk)\n",
    "    \n",
    "    rgb_coarse=None\n",
    "    accumulation_map=None\n",
    "    rgb_coarse = torch.sum(weights.unsqueeze(-1) * rgb, -2)\n",
    "    accumulation_map = torch.sum(weights, -1)\n",
    "\n",
    "    if blend_alpha:\n",
    "        rgb_coarse = rgb_coarse + (1. - accumulation_map.unsqueeze(-1)) \n",
    "    \n",
    "    rgb_fine = None\n",
    "\n",
    "    if num_samples_fine != 0:\n",
    "        with torch.no_grad():\n",
    "            bins = (z_coords[..., 1:] + z_coords[..., :-1]) / 2\n",
    "            z_fine = get_fine_samples(bins, weights[..., 1:-1], \n",
    "                                      num_samples_fine, stratify=stratify)\n",
    "\n",
    "        total_samples = num_samples_coarse + num_samples_fine\n",
    "        z_coords, _ = torch.sort(torch.cat((z_coords, z_fine), dim=-1), dim=-1)\n",
    "\n",
    "        weights, rgb = get_weights_from_samples(models[1], rays, z_coords,\n",
    "                                    total_samples, emb_opts, regularize, chunk)\n",
    "\n",
    "        rgb_fine = torch.sum(weights.unsqueeze(-1) * rgb, -2)\n",
    "        accumulation_map = torch.sum(weights, -1)\n",
    "        if blend_alpha:\n",
    "            rgb_fine = rgb_fine + (1. - accumulation_map.unsqueeze(-1)) \n",
    "    \n",
    "    depth_map=None\n",
    "    depth_map = torch.sum(weights * z_coords, -1)\n",
    "\n",
    "    return rgb_coarse, rgb_fine, depth_map, accumulation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Os-oQLTdjZzG"
   },
   "outputs": [],
   "source": [
    "class NeRF_NN(nn.Module):\n",
    "    def __init__(self, layers=8, neurons=256, skips=[4], \n",
    "                 emb_opts = (6, 6, False)):\n",
    "        super().__init__()\n",
    "        self.hidden_layers = layers\n",
    "        self.hidden_neurons = neurons\n",
    "        self.skips = skips\n",
    "        emb_size_pos, emb_size_dir, keep_original = emb_opts\n",
    "        if keep_original:\n",
    "            self.input_neurons = 3 + 6 * emb_size_pos\n",
    "            self.dir_neurons = 3 + 6 * emb_size_dir\n",
    "        else:\n",
    "            self.input_neurons = 6 * emb_size_pos\n",
    "            self.dir_neurons = 6 * emb_size_dir\n",
    "\n",
    "        # Stratul de input\n",
    "        self.input_layer = nn.Linear(self.input_neurons, neurons)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(neurons, neurons) for i in range(layers - 1)])\n",
    "\n",
    "        # Adaugare skip connection\n",
    "        for idx in skips:\n",
    "            self.layers[idx] = nn.Linear(self.input_neurons + neurons, neurons)\n",
    "\n",
    "        # Straturi intermediare speciale\n",
    "        self.density_layer = nn.Linear(neurons, 1)\n",
    "        self.feature_layer1 = nn.Linear(neurons, neurons)\n",
    "        self.feature_layer2 = nn.Linear(neurons + self.dir_neurons, neurons // 2)\n",
    "        \n",
    "        # Stratul final, din care rezulta culoarea\n",
    "        self.output_layer = nn.Linear(neurons // 2, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        pts, dir = torch.split(x, [self.input_neurons, self.dir_neurons], dim=-1)\n",
    "\n",
    "        input = pts.clone().detach()\n",
    "        # input = pts\n",
    "        val = F.relu(self.input_layer(input))\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            val = F.relu(layer(val))\n",
    "            if i + 1 in self.skips:\n",
    "                val = torch.cat([input, val], dim=-1)\n",
    "\n",
    "        # Ultimul strat nu are activare\n",
    "        # Separam stratul intermediar\n",
    "        sigma = self.density_layer(val)\n",
    "        feature_vector = self.feature_layer1(val)\n",
    "        feature_vector = torch.cat([feature_vector, dir], dim=-1)\n",
    "\n",
    "        val = F.relu_(self.feature_layer2(feature_vector))\n",
    "        rgb = self.output_layer(val)\n",
    "        outputs = torch.cat([rgb, sigma], dim=-1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi-1qr4HGKPz"
   },
   "outputs": [],
   "source": [
    "def train_batch(models, data, loss_func, optimizer, params):\n",
    "    for model in models:\n",
    "        model.train()\n",
    "    \n",
    "    batch_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    rays = data['ray'].to(device)\n",
    "    rgb_true = data['rgb'].to(device)\n",
    "\n",
    "    rgb_c, rgb_f, depth, acc = render_rays(models, rays, params)\n",
    "\n",
    "    if rgb_f is None:\n",
    "        loss = loss_func(rgb_c, rgb_true)\n",
    "    else:\n",
    "        loss = loss_func(rgb_c, rgb_true) + loss_func(rgb_f, rgb_true)\n",
    "    \n",
    "    # avoid .item when possible: \n",
    "    # https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "    batch_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU7YVUKrIFYZ"
   },
   "outputs": [],
   "source": [
    "def eval_epoch(models, dataloader, loss_func, params):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    epoch_loss = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader)):\n",
    "            rays = data['ray'].to(device)\n",
    "            rgb_true = data['rgb'].to(device)\n",
    "\n",
    "            rgb_c, rgb_f, depth, acc = render_rays(models, rays, params)\n",
    "\n",
    "            if rgb_f is None:\n",
    "                loss = loss_func(rgb_c, rgb_true)\n",
    "            else:\n",
    "                loss = loss_func(rgb_c, rgb_true) + loss_func(rgb_f, rgb_true)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZTXEK2NK-Sp"
   },
   "outputs": [],
   "source": [
    "def eval_image_sample(models, img_data, loss_func, params, ndc=False, plot=True,\n",
    "                      iter=None, batchify=False, b_size=None):\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pose = img_data['pose']\n",
    "        rgb_true = img_data['image']\n",
    "        H, W, focal = img_data['hwf']\n",
    "\n",
    "        rays = extract_rays(H, W, focal, pose)\n",
    "        if ndc:\n",
    "            rays = convert_to_ndc(H, W, focal, 1., rays)\n",
    "        rays = rays.to(device)\n",
    "\n",
    "        # rgb_t_np = rgb_true.numpy()\n",
    "\n",
    "        rgb_c = []\n",
    "        rgb_f = []\n",
    "        if batchify:\n",
    "            for i in range(0, rays.shape[0], b_size):\n",
    "                p_c, p_f, depth, acc = render_rays(models, rays[i:i+b_size], params)\n",
    "                rgb_c.append(p_c)\n",
    "                rgb_f.append(p_f)\n",
    "\n",
    "            rgb_c = torch.cat(rgb_c)\n",
    "            if p_f is not None:\n",
    "                rgb_f = torch.cat(rgb_f)\n",
    "            else:\n",
    "                rgb_f = None\n",
    "        else:\n",
    "            rgb_c, rgb_f, depth, acc = render_rays(models, rays, params)\n",
    "        \n",
    "        rgb_c = rgb_c.cpu().reshape(H, W, 3)\n",
    "        loss = loss_func(rgb_c, rgb_true)\n",
    "        psnr_c = -10 * np.log(loss) / np.log(10)\n",
    "\n",
    "        # rgb_c_np = rgb_c.numpy()\n",
    "        # psnr_c = tf.image.psnr(rgb_t_np, rgb_c_np, max_val=1.)\n",
    "        # ssim_c = tf.image.ssim(rgb_t_np, rgb_c_np, max_val=1.)\n",
    "\n",
    "        psnr_f = None\n",
    "        if rgb_f is not None:\n",
    "            rgb_f = rgb_f.cpu().reshape(H, W, 3)\n",
    "            loss_f = loss_func(rgb_f, rgb_true) \n",
    "            loss += loss_f \n",
    "            psnr_f = -10 * np.log(loss_f) / np.log(10)\n",
    "\n",
    "            # rgb_f_np = rgb_f.numpy()\n",
    "            # psnr_f = tf.image.psnr(rgb_t_np, rgb_f_np, max_val=1.)\n",
    "            # ssim_f = tf.image.ssim(rgb_t_np, rgb_f_np, max_val=1.)\n",
    "\n",
    "        if plot:\n",
    "            plot_image(rgb_c, rgb_f, rgb_true, iter)\n",
    "\n",
    "        # print(\"psnrs:\", psnr_c, psnr_f)\n",
    "        # print(\"ssims:\", ssim_c, ssim_f)\n",
    "\n",
    "        # psnr = psnr_f if psnr_f > psnr_c else psnr_c\n",
    "        # ssim = ssim_f if ssim_f > ssim_c else ssim_c\n",
    "\n",
    "        return psnr_c, psnr_f\n",
    "        # return rgb_c_np, rgb_f_np, psnr, ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNhxnLh-OGnc"
   },
   "outputs": [],
   "source": [
    "def plot_image(rgb_c, rgb_f=None, rgb_true=None, iter=None):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    num_subplots = 1 if rgb_f is None else 2\n",
    "    num_subplots += 0 if rgb_true is None else 1\n",
    "\n",
    "    if iter is None:\n",
    "        post_title = \"\"\n",
    "    else:\n",
    "        post_title = \", Iteration: \" + str(iter)\n",
    "\n",
    "    plt.subplot(1, num_subplots, 1)\n",
    "    plt.imshow(rgb_c)\n",
    "    plt.title(\"Coarse\" + post_title)\n",
    "\n",
    "    if rgb_f is not None:\n",
    "        plt.subplot(1, num_subplots, 2)\n",
    "        plt.imshow(rgb_f)\n",
    "        plt.title(\"Fine\" + post_title)\n",
    "    \n",
    "    if rgb_true is not None:\n",
    "        plt.subplot(1, num_subplots, num_subplots)\n",
    "        plt.imshow(rgb_true)\n",
    "        plt.title(\"Original\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvj5iZMJwrSW"
   },
   "source": [
    "# Hiperparametrii si initializarea datelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z1AZW8u2c7t"
   },
   "outputs": [],
   "source": [
    "embedding_size_pos = 10\n",
    "embedding_size_dir = 4\n",
    "embed_func = positional_encoding\n",
    "embed_options = (embedding_size_pos, embedding_size_dir, False)\n",
    "\n",
    "n1 = 64\n",
    "n2 = 128\n",
    "\n",
    "b_size = 5 * 512\n",
    "downscale = 8\n",
    "blender_model = 'lego'\n",
    "llff_model = 'fern'\n",
    "custom_model = 'birou'\n",
    "\n",
    "blender_path = '/gdrive/MyDrive/Nerf/NeRF_Data/nerf_synthetic/'\n",
    "llff_path = '/gdrive/MyDrive/Nerf/NeRF_Data/nerf_llff_data/'\n",
    "custom_path = '/gdrive/MyDrive/Nerf/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpJy_UtxlU9W"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models_parameters = []\n",
    "\n",
    "model_coarse = NeRF_NN(emb_opts=embed_options).to(device)\n",
    "models.append(model_coarse)\n",
    "models_parameters.append({'params': model_coarse.parameters()})\n",
    "\n",
    "if n2 > 0:\n",
    "    model_fine = NeRF_NN(emb_opts=embed_options).to(device)\n",
    "    models_parameters.append({'params': model_fine.parameters()})\n",
    "    models.append(model_fine)\n",
    "\n",
    "optimizer = torch.optim.Adam(models_parameters, lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.4, \n",
    "                                                       verbose=True)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3CuLiLsLIj6"
   },
   "outputs": [],
   "source": [
    "# trainset = BlenderRaysDataset('/gdrive/MyDrive/Nerf/NeRF_Data/nerf_synthetic/' + blender_model,\n",
    "#                       downscale_factor=downscale, blend_alpha=True)\n",
    "# trainloader = DataLoader(trainset, batch_size=b_size, shuffle=True, \n",
    "#                          num_workers=2, pin_memory=True)\n",
    "\n",
    "# validset = BlenderRaysDataset('/gdrive/MyDrive/Nerf/NeRF_Data/nerf_synthetic/' + blender_model,\n",
    "#                       downscale_factor=downscale, role='val', blend_alpha=True)\n",
    "# validloader = DataLoader(validset, batch_size=b_size * 2, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoj2LZ8mCmQY"
   },
   "outputs": [],
   "source": [
    "# testimages = BlenderImageDataset('/gdrive/MyDrive/Nerf/NeRF_Data/nerf_synthetic/' + blender_model,\n",
    "#                       downscale_factor=downscale, role='test', blend_alpha=True)\n",
    "# testimgloader = DataLoader(testimages, batch_size=None, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvwcjY-KM-wZ"
   },
   "outputs": [],
   "source": [
    "# if not os.path.exists('train' + str(downscale) + '.tar'):\n",
    "#     trainset = LLFFDataset('/gdrive/MyDrive/Nerf/NeRF_Data/nerf_llff_data/' + llff_model,\n",
    "#                           downscale_factor=downscale, role='train')\n",
    "#     torch.save(trainset, 'train' + str(downscale) + '.tar')\n",
    "# else:\n",
    "#     trainset = torch.load('train' + str(downscale) + '.tar')\n",
    "# trainloader = DataLoader(trainset, batch_size=b_size, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIYW9_PiNOFe"
   },
   "outputs": [],
   "source": [
    "# testimages = LLFFDataset('/gdrive/MyDrive/Nerf/NeRF_Data/nerf_llff_data/' + llff_model,\n",
    "#                       downscale_factor=downscale, role='test')\n",
    "# testimgloader = DataLoader(testimages, batch_size=None, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jjv7ejrWdWwL",
    "outputId": "391d51a1-ac06-465a-838d-436868f8b779"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('train' + str(downscale) + '.tar'):\n",
    "    trainset = LLFFDataset(custom_path + custom_model,\n",
    "                          downscale_factor=downscale, role='train')\n",
    "    torch.save(trainset, 'train' + str(downscale) + '.tar')\n",
    "else:\n",
    "    trainset = torch.load('train' + str(downscale) + '.tar')\n",
    "trainloader = DataLoader(trainset, batch_size=b_size, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19Oxi9vMdlUt",
    "outputId": "931b7c01-bda5-4d1c-a10a-c2d761603169",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "testimages = LLFFDataset(custom_path + custom_model,\n",
    "                      downscale_factor=downscale, role='test')\n",
    "testimgloader = DataLoader(testimages, batch_size=None, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "JbC6GIoKDeOh",
    "outputId": "8c5beac1-b6c0-44f7-fc51-e92698aab1e0"
   },
   "outputs": [],
   "source": [
    "for img in testimgloader:\n",
    "    imdata = img\n",
    "    break\n",
    "\n",
    "plt.imshow(imdata['image'])\n",
    "print(imdata['pose'])\n",
    "print(imdata['hwf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpCUOas4d0mz"
   },
   "outputs": [],
   "source": [
    "# for img in testimgloader:\n",
    "#     eval_image_sample(models, img, loss_func, params_llff_eval, ndc=True, plot=True, batchify=True, b_size=12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4y_zHJvMnTu",
    "outputId": "dc6c02d3-9cd2-4351-bbdf-a17d86678270"
   },
   "outputs": [],
   "source": [
    "print(len(trainloader))\n",
    "epochs = 20\n",
    "metrics = { 'loss_train': [], 'iternum': []}\n",
    "i_metrics = 500\n",
    "# i_save_model = 11200\n",
    "offset = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eobG1mGlMGkV"
   },
   "source": [
    "# Functionalitati pentru antrenament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrqFQqCAbcDD"
   },
   "outputs": [],
   "source": [
    "# metrics_path = \\\n",
    "# '/gdrive/MyDrive/Nerf/Models/nerf_synthetic/metrics/metrics_' + blender_model +\\\n",
    "# '_' + 'full_56750.pt'\n",
    "\n",
    "# checkpoint_path = \\\n",
    "# '/gdrive/MyDrive/Nerf/Models/nerf_synthetic/checkpoints/nerf_' +\\\n",
    "# blender_model + '_' + 'full' + '_'\n",
    "\n",
    "# image_path = \\\n",
    "# '/gdrive/MyDrive/Nerf/Models/nerf_synthetic/outputs/' + blender_model +\\\n",
    "#  '_test_'\n",
    "\n",
    "metrics_path = \\\n",
    "'/gdrive/MyDrive/Nerf/Models/nerf_llff/metrics/metrics_' + custom_model +\\\n",
    "'_8_128.pt'\n",
    "\n",
    "checkpoint_path = \\\n",
    "'/gdrive/MyDrive/Nerf/Models/nerf_llff/checkpoints/nerf_' +\\\n",
    "custom_model + '_8_128_'\n",
    "\n",
    "image_path = \\\n",
    "'/gdrive/MyDrive/Nerf/Models/nerf_llff/outputs/' + custom_model +\\\n",
    " '_8_128_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blRtVxe6bw2l",
    "outputId": "652ae5f6-a08a-4177-919f-97742c0ab1cc"
   },
   "outputs": [],
   "source": [
    "print(metrics_path)\n",
    "print(checkpoint_path)\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko4hGKcok4Xj"
   },
   "outputs": [],
   "source": [
    "metrics = torch.load(metrics_path)\n",
    "# 81600 pt llff\n",
    "checkpoint = torch.load(checkpoint_path + '63710.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "50Bm3YpoTiNE",
    "outputId": "bc1756e7-476e-4699-f033-8cdd406c517b"
   },
   "outputs": [],
   "source": [
    "plt.plot(metrics['iternum'][-20:], metrics['loss_train'][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mRsTMkwa-mb",
    "outputId": "89069cf5-32e4-4712-ee26-c5c98e9b36be"
   },
   "outputs": [],
   "source": [
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b55Rve0XlTGG",
    "outputId": "d37c42d0-4f7f-4ca0-e097-bce39c0b2025"
   },
   "outputs": [],
   "source": [
    "print(checkpoint['scheduler_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qckTOHFAlG9Q",
    "outputId": "76bf426c-7187-408b-de39-08f869af190a"
   },
   "outputs": [],
   "source": [
    "models[0].load_state_dict(checkpoint['model_coarse_state_dict'])\n",
    "models[1].load_state_dict(checkpoint['model_fine_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "offset = checkpoint['iternum']\n",
    "print(offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l7Wlpowof4-I",
    "outputId": "1e2b4ca5-9d7f-4b93-d51a-0df12bf033f4"
   },
   "outputs": [],
   "source": [
    "print(np.array(metrics['iternum'])[np.argsort(metrics['loss_train'])][:10])\n",
    "print(np.sort(metrics['loss_train'])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rX4VsSQ_pIiE",
    "outputId": "f106b0b6-e512-49f8-eb74-3180379791be"
   },
   "outputs": [],
   "source": [
    "print(scheduler.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfZVlvgBeAOx"
   },
   "outputs": [],
   "source": [
    "# near, far, num_samples_coarse, num_samples_fine, emb_opts, \\\n",
    "    #   blend_alpha, stratify: bool, regularize:float, chunk\n",
    "\n",
    "# params_blender_train = (2, 6, n1, n2, embed_options, True, True, 0., None)\n",
    "# params_blender_eval = (2, 6, n1, n2, embed_options, True, True, 0., 1024*32*4)\n",
    "\n",
    "params_llff_train = (0., 1., n1, n2, embed_options, False, True, 1., None)\n",
    "params_llff_eval = (0., 1., n1, n2, embed_options, False, False, 0., None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "33b289bfbd8649498b9d4efbdda9e106",
      "d7eb749e4d2c4deba0ed7751481b7870",
      "4f72c7cc7067451ea534b9beaebff367",
      "185ee94a7e2043b8915b8ac80e6b3804",
      "ace775105fde4497aab188682f6ce68e",
      "ca142305918447168deb1d1c6741fdb5",
      "23785aaf8f844bc3be21a8eb54417c1f",
      "40772ab4271d44aea902d40b0235f66d"
     ]
    },
    "id": "D7A5sWhTNx_l",
    "outputId": "b57e135b-b8b1-4835-832b-75e668984e5c"
   },
   "outputs": [],
   "source": [
    "miniepoch_loss = 0. # metrics['loss_train'][-1] *(offset - metrics['iternum'][-1]) / 500\n",
    "for i in tqdm(range(epochs)):\n",
    "    for j, data in enumerate(trainloader):\n",
    "        batch_loss = train_batch(models, data, loss_func, optimizer, \n",
    "                                 params_llff_train)\n",
    "        miniepoch_loss += batch_loss\n",
    "\n",
    "        iteration = i * len(trainloader) + j\n",
    "        first_flag = (iteration != 0)\n",
    "        iteration += offset\n",
    "\n",
    "        if first_flag and iteration % i_metrics == 0:\n",
    "            scheduler.step(miniepoch_loss)\n",
    "            \n",
    "            metrics['iternum'].append(iteration)\n",
    "            metrics['loss_train'].append(miniepoch_loss)\n",
    "            miniepoch_loss = 0.\n",
    "            plt.plot(metrics['iternum'][-20:], metrics['loss_train'][-20:])\n",
    "            plt.show()\n",
    "            \n",
    "    # if first_flag and iteration % i_save_model == 0:\n",
    "    print('Salvare model...')\n",
    "    torch.save(metrics, metrics_path)\n",
    "    torch.save({\n",
    "        'model_coarse_state_dict': models[0].state_dict(),\n",
    "        'model_fine_state_dict': models[1].state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'iternum': iteration\n",
    "    }, checkpoint_path + str(iteration) + '.tar')\n",
    "\n",
    "        # if j % i_plot==0:\n",
    "            # iteration = i * len(trainloader) + j\n",
    "            # psnr_c, psnr_f = eval_image_sample(model, imdata, loss_func, \n",
    "            #                 params_llff_eval, iter=iteration)\n",
    "            \n",
    "            # psnrs_c.append(psnr_c)\n",
    "            # psnrs_f.append(psnr_f)\n",
    "            # iternums.append(iteration)\n",
    "            # torch.save(model.state_dict(), '/gdrive/MyDrive/Nerf/Models/nerf_synthetic/nerf_blender_ship_' + str(iteration) + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_RPUddsa7qp"
   },
   "outputs": [],
   "source": [
    "# miniepoch_loss = 0.\n",
    "# for i in tqdm(range(epochs)):\n",
    "#     for j, data in enumerate(trainloader):\n",
    "#         batch_loss = train_batch(model, data, loss_func, optimizer, \n",
    "#                                  params_blender_train)\n",
    "#         miniepoch_loss += batch_loss\n",
    "        \n",
    "#         iteration = i * len(trainloader) + j\n",
    "#         first_flag = (iteration != 0)\n",
    "#         iteration += offset\n",
    "\n",
    "#         if first_flag and iteration % i_metrics == 0:\n",
    "#             scheduler.step(miniepoch_loss)\n",
    "            \n",
    "#             metrics['iternum'].append(iteration)\n",
    "#             metrics['loss_train'].append(miniepoch_loss)\n",
    "#             miniepoch_loss = 0.\n",
    "#             plt.plot(metrics['iternum'][-20:], metrics['loss_train'][-20:])\n",
    "#             plt.show()\n",
    "#             torch.save(metrics, metrics_path + str(iteration) + '.pt')\n",
    "            \n",
    "#         if first_flag and iteration % i_save_model == 0:\n",
    "#             print('Salvare model...')\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'iternum': iteration\n",
    "#             }, checkpoint_path + str(iteration) + '.tar')\n",
    "\n",
    "#         # if j % i_plot==0:\n",
    "#         #     iteration = i * len(trainloader) + j\n",
    "            \n",
    "#         #     psnr_c, psnr_f = eval_image_sample(model, imdata, loss_func, \n",
    "#         #                     params_blender_eval, iter=iteration)\n",
    "            \n",
    "#         #     psnrs_c.append(psnr_c)\n",
    "#         #     psnrs_f.append(psnr_f)\n",
    "#         #     iternums.append(iteration)\n",
    "#             # torch.save(model.state_dict(), '/gdrive/MyDrive/Nerf/Models/nerf_synthetic/nerf_blender_' + blender_model + '_' + str(iteration) + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Clkry2YPL_f3"
   },
   "source": [
    "# Metrici de testare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AaB-d0cm-i_",
    "outputId": "1f4e5a70-6084-4a70-dd55-610816d18412"
   },
   "outputs": [],
   "source": [
    "results = {'psnrs': [], 'ssims': []}\n",
    "# results = torch.load(metrics_path + 'results.pt')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arczUfZ03Yd1"
   },
   "outputs": [],
   "source": [
    "# _, _, files = next(os.walk('/gdrive/MyDrive/Nerf/Models/nerf_synthetic/outputs/'))\n",
    "\n",
    "# print(files)\n",
    "# for i in range(200):\n",
    "#     if os.path.exists(image_path + str(i) +'.png'):\n",
    "#       print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwbDTSEPF_s6"
   },
   "outputs": [],
   "source": [
    "# for key in results:\n",
    "#   print(np.mean(results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Y1U7Zqi5dmW"
   },
   "outputs": [],
   "source": [
    "# for i, img in enumerate(testimgloader):\n",
    "#     if os.path.exists(image_path + str(i) +'.png'):\n",
    "#       print(\"Skipping image \" + str(i))\n",
    "#       continue\n",
    "#     rgb, psnr, ssim = eval_image_sample(model, img, loss_func, params_blender_train, \n",
    "#                                         False, batchify=True, b_size=12*1024)\n",
    "\n",
    "#     imageio.imwrite(image_path + str(i) + '.png', (rgb * 255).astype(np.uint8))\n",
    "#     results['psnrs'].append(psnr.numpy())\n",
    "#     results['ssims'].append(ssim.numpy())\n",
    "\n",
    "#     print(results)\n",
    "#     torch.save(results, metrics_path + 'results.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P93itPKq1Fgh"
   },
   "outputs": [],
   "source": [
    "# scores = []\n",
    "\n",
    "# for i, img in enumerate(trainset.images):\n",
    "#     scores.append(tf.image.psnr(tf.convert_to_tensor(img.numpy()), tf.convert_to_tensor(frames_rgb[i] / 255, dtype=tf.float32), 1.))\n",
    "\n",
    "# print([score.numpy() for score in scores][::])\n",
    "# print(np.mean(scores[::]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rn4srgB0dWFs"
   },
   "outputs": [],
   "source": [
    "# for i, img in enumerate(testimgloader):\n",
    "#     # if os.path.exists(image_path + str(i) +'.png'):\n",
    "#     #   print(\"Skipping image \" + str(i))\n",
    "#     #   continue\n",
    "#     rgb_c, rgb_f, psnr, ssim = eval_image_sample(models, img, loss_func, params_llff_eval, \n",
    "#                                         ndc=True, plot=False, batchify=True, b_size=12*1024)\n",
    "\n",
    "#     imageio.imwrite(image_path + str(i) + '_c.png', (rgb_c * 255).astype(np.uint8))\n",
    "#     imageio.imwrite(image_path + str(i) + '_f.png', (rgb_f * 255).astype(np.uint8))\n",
    "\n",
    "#     results['psnrs'].append(psnr.numpy())\n",
    "#     results['ssims'].append(ssim.numpy())\n",
    "\n",
    "#     print(results)\n",
    "#     torch.save(results, metrics_path + 'results.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PIRXKJ1iyACf",
    "outputId": "84afd54a-7a91-429a-88f0-654b0f3c28f0"
   },
   "outputs": [],
   "source": [
    "# print(np.mean(results['psnrs']))\n",
    "# print(np.mean(results['ssims']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R64-UrvVhPmS",
    "outputId": "7388b8b5-f2ed-4cf3-a796-0a074d07c60e"
   },
   "outputs": [],
   "source": [
    "# image0_ph = tf.placeholder(tf.float32)\n",
    "# image1_ph = tf.placeholder(tf.float32)\n",
    "\n",
    "# lpips_func = lpips_tf.lpips(image0_ph, image1_ph, model='net-lin', net='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FAX4Mvy0MxtC"
   },
   "outputs": [],
   "source": [
    "# gt_path = '/gdrive/MyDrive/Nerf/NeRF_Data/nerf_llff_data/' + llff_model + '/images_4'\n",
    "#  _, _, a = next(os.walk(gt_path))\n",
    "# a.sort()\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZhrd2XLOT2i"
   },
   "outputs": [],
   "source": [
    "# def clipify(matrix):\n",
    "#     return np.clip(matrix / 255, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpOe8qw1MjYL",
    "outputId": "e318592c-f70d-4232-bebf-9c1322e68c95"
   },
   "outputs": [],
   "source": [
    "# trues=[]\n",
    "# for fname in a:\n",
    "#   trues.append(clipify(imageio.imread(os.path.join(gt_path, fname))))\n",
    "\n",
    "# trues = np.stack(trues, axis=0)\n",
    "# print(trues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "md-LVWyVFWze",
    "outputId": "bf9af8bb-245e-4d7d-da09-4109242061d9"
   },
   "outputs": [],
   "source": [
    "# print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNdOj2QrFdO4"
   },
   "outputs": [],
   "source": [
    "# # coarses = []\n",
    "# fines=[]\n",
    "# for i in range(200):\n",
    "#   # coarse_path = image_path + str(i) + '_c.png'\n",
    "#   fine_path = image_path + str(i) + '.png'\n",
    "\n",
    "#   # img_c = imageio.imread(coarse_path)\n",
    "#   # coarses.append(clipify(img_c))\n",
    "\n",
    "#   img_f = imageio.imread(fine_path)\n",
    "#   fines.append(clipify(img_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwa__0c8HSRu"
   },
   "outputs": [],
   "source": [
    "# imc = np.stack(coarses, axis=0)\n",
    "# imf = np.stack(fines, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTzmSG8PYuwd",
    "outputId": "52217718-ef4d-42f1-818c-c383fa238915"
   },
   "outputs": [],
   "source": [
    "# print(testimages.images.shape, testimages.images.min(), testimages.images.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkzrlCbgHxyr",
    "outputId": "7d543ada-6f3d-495e-8c34-f9931a9ea5cf"
   },
   "outputs": [],
   "source": [
    "# print(imc.shape, imf.shape, imc.min(), imc.max(), imf.min(), imf.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "N0v43bJ3O2Yv",
    "outputId": "a6898dc4-891e-421c-94f2-9272e942630b"
   },
   "outputs": [],
   "source": [
    "# plt.imshow(imf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnk64eb0It3u",
    "outputId": "4e178a3d-9298-46c6-b7bc-05d65c43e4c3"
   },
   "outputs": [],
   "source": [
    "distance = np.zeros(imf.shape[0])\n",
    "print(distance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jv3OEakIFGi0",
    "outputId": "146ff70a-be64-4938-8c7d-d53bd05e6481"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    for i in range(200):\n",
    "      distance[i] = session.run(lpips_func, \n",
    "                feed_dict={image0_ph: testimages.images[i],\n",
    "                           image1_ph: imf[i]})\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcZxMwtIPWZr"
   },
   "outputs": [],
   "source": [
    "dist_fine = np.array(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xuQCuzktPhMH",
    "outputId": "6f095dec-c2ea-4daa-9aaf-0a8c29d4abbb"
   },
   "outputs": [],
   "source": [
    "print(dist_coarse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbN6lEoWPq0r",
    "outputId": "77ed9918-ab2c-4b66-d098-adae6ab48ddc"
   },
   "outputs": [],
   "source": [
    "print(dist_fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqKXkk_4uPod"
   },
   "source": [
    "# Generarea rezultatelor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "W1bGovWqg7Ai",
    "outputId": "85fd0f89-7d78-4ae8-d837-64094cd9fade"
   },
   "outputs": [],
   "source": [
    "# eval_image_sample(model, imdata, loss_func, params_blender_train, True, batchify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5G3WhW1ph_AZ"
   },
   "outputs": [],
   "source": [
    "# translate_z = lambda z: np.array([\n",
    "#     [1, 0, 0, 0],\n",
    "#     [0, 1, 0, 0],\n",
    "#     [0, 0, 1, z],\n",
    "#     [0, 0, 0, 1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# rotate_x = lambda ang: np.array([\n",
    "#     [1,           0,            0, 0],\n",
    "#     [0, np.cos(ang), -np.sin(ang), 0],\n",
    "#     [0, np.sin(ang),  np.cos(ang), 0],\n",
    "#     [0,           0,            0, 1]\n",
    "# ], dtype=np.float32)\n",
    "\n",
    "# rotate_y = lambda ang: np.array([\n",
    "#     [np.cos(ang), 0, -np.sin(ang), 0],\n",
    "#     [          0, 1,            0, 0],\n",
    "#     [np.sin(ang), 0,  np.cos(ang), 0],\n",
    "#     [0,           0,            0, 1]\n",
    "# ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6SKHOa-xiYY"
   },
   "outputs": [],
   "source": [
    "# def generate_pose_polar_coords(phi, theta, radius):\n",
    "#     cam_to_world = translate_z(radius)\n",
    "#     cam_to_world = rotate_y(theta) @ rotate_x(phi)  @ cam_to_world\n",
    "    \n",
    "#     # Inversam coordonatele pe x, interschimbam axele y si z\n",
    "#     cam_to_world = \\\n",
    "#         np.array([[-1, 0, 0, 0], \n",
    "#                   [ 0, 0, 1, 0], \n",
    "#                   [ 0, 1, 0, 0], \n",
    "#                   [ 0, 0, 0, 1]], dtype=np.float32) @ cam_to_world\n",
    "\n",
    "#     return cam_to_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "0e33f51b62564d04ba426b4a22e056a1",
      "cffc3e47dfbf4e57b9d5c8da174b2d78",
      "8c1ca825efac430d9e632cbdbda6ce52",
      "e58432f0695b42e9bb5f6dc14575a0e5",
      "5dba1b32f9d34b668f85c5bfc0a6cc5d",
      "9e0d17a3506643738424f27f5f54ec31",
      "350097a0d8de4eebaa1e7ca059734a04",
      "01660d5060534e1bab58146cbc668b1f"
     ]
    },
    "id": "JaD0087Zy9wX",
    "outputId": "fc16c9f4-51c1-4190-aecb-33c22c8a9202"
   },
   "outputs": [],
   "source": [
    "# frames = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for y_ang in tqdm(np.linspace(2/3 * np.pi, 2 * np.pi, 1, endpoint=False)):\n",
    "#         for model in models:\n",
    "#           model.eval()\n",
    "#         pose = generate_pose_polar_coords(np.radians(-30.), y_ang, 4.)\n",
    "\n",
    "#         rays = extract_rays(800, 800, 1111.1110311937682, torch.from_numpy(pose))\n",
    "#         rgb_c, rgb_f, depth, acc = render_rays(models, rays.to(device), params_blender_eval)\n",
    "\n",
    "#         rgb = rgb_f.cpu().reshape(800, 800, 3)\n",
    "#         frames.append((255 * np.clip(rgb.numpy(), 0, 1)).astype(np.uint8))\n",
    "\n",
    "#         depth = depth.cpu().reshape(800, 800)\n",
    "#         frames.append(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLKr74lW10je"
   },
   "outputs": [],
   "source": [
    "def create_llff_video_poses(dataset, n_img=120, n_rot=2):\n",
    "    # avg_pose = dataset.avg_pose\n",
    "\n",
    "    # (3), rotatia pe y\n",
    "    # up = normalize(np.sum(dataset.all_poses[..., 1].numpy(), axis=0))\n",
    "    up = np.array([0., 1., 0.])\n",
    "\n",
    "    f_depth = 3.5\n",
    "    rads = np.percentile(np.abs(dataset.all_poses[..., 3]), 90, axis=0)\n",
    "\n",
    "    video_poses = []\n",
    "    # rads = np.array(list(rads) + [1.])\n",
    "\n",
    "    for angle in np.linspace(0, 2 * n_rot * np.pi, n_img, endpoint=False):\n",
    "        # position = np.dot(avg_pose, np.array([np.cos(angle), \n",
    "        #                                      -np.sin(angle), \n",
    "        #                                      -np.sin(angle / 2),\n",
    "        #                                       1]) * rads)\n",
    "        position = np.array([np.cos(angle), \n",
    "                            -np.sin(angle), \n",
    "                            -np.sin(angle / 2)]) * rads\n",
    "\n",
    "        # z = normalize(position - np.dot(avg_pose, np.array([0, 0, -f_depth, 1])))\n",
    "        z = normalize(position - np.array([0, 0, -f_depth]))\n",
    "\n",
    "\n",
    "        x = normalize(np.cross(up, z))\n",
    "        y = normalize(np.cross(z, x))\n",
    "\n",
    "        video_poses.append(np.column_stack((x, y, z, position)))\n",
    "    \n",
    "    return torch.tensor(video_poses, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB6z9mjSYR2l"
   },
   "outputs": [],
   "source": [
    "params_llff_eval = (0., 1., n1, n2, embed_options, False, True, 0., None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcyPbQnF2Dpz"
   },
   "outputs": [],
   "source": [
    "poses = create_llff_video_poses(trainset)\n",
    "print(poses.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njd-_NAs2GAM"
   },
   "outputs": [],
   "source": [
    "frames_rgb = []\n",
    "frames_d = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    skipped = 0\n",
    "    for j, pose in tqdm(enumerate(poses)):\n",
    "        if (os.path.exists(image_path + 'rgb_' + str(j) + '.png')):\n",
    "            print(\"Skipping \" + str(j))\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        # pose = trainset.poses[0]\n",
    "        H, W, focal = trainset.hwf\n",
    "\n",
    "        rays = convert_to_ndc(H, W, focal, 1., extract_rays(H, W, focal, pose)).to(device)\n",
    "\n",
    "        # co=[]\n",
    "        fi=[]\n",
    "        de=[]\n",
    "        batch_eval = 12500\n",
    "        for i in range(0, H * W, batch_eval):\n",
    "            rgb_c, rgb_f, depth, acc = render_rays(models, \n",
    "                                                rays[i:i+batch_eval], \n",
    "                                                params_llff_eval)\n",
    "            # co.append(rgb_c)\n",
    "            fi.append(rgb_f)\n",
    "            de.append(depth)\n",
    "\n",
    "        # rgb_c = torch.cat(co)\n",
    "        rgb_f = torch.cat(fi)\n",
    "        depth = torch.cat(de)\n",
    "\n",
    "        # rgb_c, rgb_f, depth, acc = render_rays(models, rays, params_llff_eval)\n",
    "\n",
    "        rgb = rgb_f.cpu().reshape(H, W, 3)\n",
    "        frames_rgb.append((255 * np.clip(rgb.numpy(), 0, 1)).astype(np.uint8))\n",
    "\n",
    "        bw = depth.cpu().reshape(H, W)\n",
    "        frames_d.append((255 * (1. - np.clip(bw.numpy(), 0, 1))).astype(np.uint8))\n",
    "\n",
    "        imageio.imwrite(image_path + 'rgb_' + str(j) + '.png', frames_rgb[j - skipped])\n",
    "        imageio.imwrite(image_path + 'depth_' + str(j) + '.png', frames_d[j - skipped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aq78QtKZ1vVz"
   },
   "outputs": [],
   "source": [
    "# for i, img in enumerate(testimgloader):\n",
    "#     # if os.path.exists(image_path + str(i) +'.png'):\n",
    "#     #   print(\"Skipping image \" + str(i))\n",
    "#     #   continue\n",
    "#     rgb_c, rgb_f, psnr, ssim = eval_image_sample(models, img, loss_func, params_llff_eval, \n",
    "#                                         ndc=True, plot=False, batchify=True, b_size=12*1024)\n",
    "\n",
    "#     imageio.imwrite(image_path + str(i) + '_c.png', (rgb_c * 255).astype(np.uint8))\n",
    "#     imageio.imwrite(image_path + str(i) + '_f.png', (rgb_f * 255).astype(np.uint8))\n",
    "\n",
    "#     results['psnrs'].append(psnr.numpy())\n",
    "#     results['ssims'].append(ssim.numpy())\n",
    "\n",
    "#     print(results)\n",
    "#     torch.save(results, metrics_path + 'results.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHEUTTHecBEW"
   },
   "outputs": [],
   "source": [
    "frames = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    pose = testimages.poses[3]\n",
    "    H, W, focal = testimages.hwf\n",
    "\n",
    "    rays = convert_to_ndc(H, W, focal, 1., extract_rays(H, W, focal, pose)).to(device)\n",
    "\n",
    "    co=[]\n",
    "    fi=[]\n",
    "    batch_eval = 10000\n",
    "    for i in range(0, H * W, batch_eval):\n",
    "        rgb_c, rgb_f, depth, acc = render_rays(models, \n",
    "                                               rays[i:i+batch_eval], \n",
    "                                               params_llff_eval)\n",
    "        co.append(rgb_c)\n",
    "        fi.append(rgb_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-Z2hfXLeEy1"
   },
   "outputs": [],
   "source": [
    "rgb_c = torch.cat(co)\n",
    "rgb_f = torch.cat(fi)\n",
    "\n",
    "rgb = rgb_c.cpu().reshape(H, W, 3)\n",
    "frames.append((255 * np.clip(rgb.numpy(), 0, 1)).astype(np.uint8))\n",
    "\n",
    "rgb = rgb_f.cpu().reshape(H, W, 3)\n",
    "frames.append((255 * np.clip(rgb.numpy(), 0, 1)).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d1sQm-uaipx6"
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcpPcEmrWYf7"
   },
   "outputs": [],
   "source": [
    "imageio.imwrite('lego_good.png', frames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "EHx0m8qzTP4f",
    "outputId": "7936c2d2-a267-428e-8f89-809e5affbf83"
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(30, 10))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frames[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(frames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "xdrRZxzwf8Zu",
    "outputId": "fa144552-3cd4-42ba-803f-6e8a893a969f"
   },
   "outputs": [],
   "source": [
    "original = testimages.images[3]#[19 * 1008 * 756:20 * 1008 * 756].reshape(H, W, 3)\n",
    "plt.imshow(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoHtxqU59fnS"
   },
   "outputs": [],
   "source": [
    "imageio.imwrite('ferndepth.png', frames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5M-9_zlai1g",
    "outputId": "cab075ee-ed2e-472c-d9bd-74636e0ec1fb"
   },
   "outputs": [],
   "source": [
    "print(peak_signal_noise_ratio(original.cpu().numpy(), frames[1] / 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEE8p09kXuQm"
   },
   "outputs": [],
   "source": [
    "f = 'ship.mp4'\n",
    "imageio.mimwrite(f, frames, fps=30, quality=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "6cin9cGnZ6FP",
    "outputId": "433ede59-8f81-4746-d088-caed39d568f6"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open('ship.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls autoplay loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "LD-weF0Zd-Aw",
    "Vtrbenu-w71W"
   ],
   "name": "Licenta PyTorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01660d5060534e1bab58146cbc668b1f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e33f51b62564d04ba426b4a22e056a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8c1ca825efac430d9e632cbdbda6ce52",
       "IPY_MODEL_e58432f0695b42e9bb5f6dc14575a0e5"
      ],
      "layout": "IPY_MODEL_cffc3e47dfbf4e57b9d5c8da174b2d78"
     }
    },
    "185ee94a7e2043b8915b8ac80e6b3804": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40772ab4271d44aea902d40b0235f66d",
      "placeholder": "",
      "style": "IPY_MODEL_23785aaf8f844bc3be21a8eb54417c1f",
      "value": " 10/20 [3:40:54&lt;3:40:51, 1325.12s/it]"
     }
    },
    "23785aaf8f844bc3be21a8eb54417c1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "33b289bfbd8649498b9d4efbdda9e106": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f72c7cc7067451ea534b9beaebff367",
       "IPY_MODEL_185ee94a7e2043b8915b8ac80e6b3804"
      ],
      "layout": "IPY_MODEL_d7eb749e4d2c4deba0ed7751481b7870"
     }
    },
    "350097a0d8de4eebaa1e7ca059734a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40772ab4271d44aea902d40b0235f66d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f72c7cc7067451ea534b9beaebff367": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": " 50%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca142305918447168deb1d1c6741fdb5",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ace775105fde4497aab188682f6ce68e",
      "value": 10
     }
    },
    "5dba1b32f9d34b668f85c5bfc0a6cc5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8c1ca825efac430d9e632cbdbda6ce52": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e0d17a3506643738424f27f5f54ec31",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5dba1b32f9d34b668f85c5bfc0a6cc5d",
      "value": 1
     }
    },
    "9e0d17a3506643738424f27f5f54ec31": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ace775105fde4497aab188682f6ce68e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ca142305918447168deb1d1c6741fdb5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cffc3e47dfbf4e57b9d5c8da174b2d78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7eb749e4d2c4deba0ed7751481b7870": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e58432f0695b42e9bb5f6dc14575a0e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01660d5060534e1bab58146cbc668b1f",
      "placeholder": "",
      "style": "IPY_MODEL_350097a0d8de4eebaa1e7ca059734a04",
      "value": " 1/1 [01:41&lt;00:00, 101.76s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
